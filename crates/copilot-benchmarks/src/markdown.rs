//! Markdown report generation
//!
//! This module generates markdown summary reports from benchmark results.

use crate::result::BenchmarkResult;
use chrono::{DateTime, Utc};

/// Configuration for markdown report generation
#[derive(Debug, Clone)]
pub struct MarkdownConfig {
    /// Title for the report
    pub title: String,
    /// Include individual target details
    pub include_details: bool,
    /// Include timestamp in header
    pub include_timestamp: bool,
    /// Show metrics in tables
    pub use_tables: bool,
}

impl Default for MarkdownConfig {
    fn default() -> Self {
        Self {
            title: "Benchmark Results Summary".to_string(),
            include_details: true,
            include_timestamp: true,
            use_tables: true,
        }
    }
}

/// Markdown report generator
pub struct MarkdownGenerator {
    config: MarkdownConfig,
}

impl Default for MarkdownGenerator {
    fn default() -> Self {
        Self::new()
    }
}

impl MarkdownGenerator {
    /// Create a new generator with default configuration
    pub fn new() -> Self {
        Self {
            config: MarkdownConfig::default(),
        }
    }

    /// Create a generator with custom configuration
    pub fn with_config(config: MarkdownConfig) -> Self {
        Self { config }
    }

    /// Generate a markdown report from benchmark results
    pub fn generate(&self, results: &[BenchmarkResult]) -> String {
        let mut md = String::new();

        // Header
        md.push_str(&format!("# {}\n\n", self.config.title));

        if self.config.include_timestamp {
            let now: DateTime<Utc> = Utc::now();
            md.push_str(&format!(
                "**Generated:** {}\n\n",
                now.format("%Y-%m-%d %H:%M:%S UTC")
            ));
        }

        // Summary statistics
        md.push_str("## Summary\n\n");
        md.push_str(&self.generate_summary(results));
        md.push('\n');

        // Results table
        if self.config.use_tables && !results.is_empty() {
            md.push_str("## Results\n\n");
            md.push_str(&self.generate_results_table(results));
            md.push('\n');
        }

        // Detailed results
        if self.config.include_details && !results.is_empty() {
            md.push_str("## Details\n\n");
            for result in results {
                md.push_str(&self.generate_detail(result));
                md.push('\n');
            }
        }

        // Footer
        md.push_str("---\n\n");
        md.push_str("*Generated by copilot-benchmarks - Canonical Benchmark Interface*\n");

        md
    }

    /// Generate summary statistics
    fn generate_summary(&self, results: &[BenchmarkResult]) -> String {
        let total = results.len();
        let successful = results.iter().filter(|r| r.is_success()).count();
        let failed = total - successful;

        let avg_duration: Option<f64> = {
            let durations: Vec<u64> = results.iter().filter_map(|r| r.duration_ms()).collect();
            if durations.is_empty() {
                None
            } else {
                Some(durations.iter().sum::<u64>() as f64 / durations.len() as f64)
            }
        };

        let min_duration = results.iter().filter_map(|r| r.duration_ms()).min();
        let max_duration = results.iter().filter_map(|r| r.duration_ms()).max();

        let mut summary = String::new();

        summary.push_str(&format!("- **Total Benchmarks:** {}\n", total));
        summary.push_str(&format!(
            "- **Successful:** {} ({:.1}%)\n",
            successful,
            if total > 0 {
                (successful as f64 / total as f64) * 100.0
            } else {
                0.0
            }
        ));
        summary.push_str(&format!("- **Failed:** {}\n", failed));

        if let Some(avg) = avg_duration {
            summary.push_str(&format!("- **Average Duration:** {:.2} ms\n", avg));
        }
        if let Some(min) = min_duration {
            summary.push_str(&format!("- **Min Duration:** {} ms\n", min));
        }
        if let Some(max) = max_duration {
            summary.push_str(&format!("- **Max Duration:** {} ms\n", max));
        }

        summary
    }

    /// Generate results table
    fn generate_results_table(&self, results: &[BenchmarkResult]) -> String {
        let mut table = String::new();

        table.push_str("| Target | Status | Duration (ms) | Timestamp |\n");
        table.push_str("|--------|--------|---------------|----------|\n");

        for result in results {
            let status = if result.is_success() {
                "✅ Pass"
            } else {
                "❌ Fail"
            };

            let duration = result
                .duration_ms()
                .map(|d| d.to_string())
                .unwrap_or_else(|| "-".to_string());

            let timestamp = result.timestamp.format("%Y-%m-%d %H:%M:%S");

            table.push_str(&format!(
                "| {} | {} | {} | {} |\n",
                result.target_id, status, duration, timestamp
            ));
        }

        table
    }

    /// Generate detailed result for a single benchmark
    fn generate_detail(&self, result: &BenchmarkResult) -> String {
        let mut detail = String::new();

        detail.push_str(&format!("### {}\n\n", result.target_id));

        let status = if result.is_success() {
            "✅ Passed"
        } else {
            "❌ Failed"
        };
        detail.push_str(&format!("**Status:** {}\n", status));
        detail.push_str(&format!(
            "**Timestamp:** {}\n",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));

        if let Some(duration) = result.duration_ms() {
            detail.push_str(&format!("**Duration:** {} ms\n", duration));
        }

        if let Some(error) = result.error() {
            detail.push_str(&format!("\n**Error:** {}\n", error));
        }

        // Additional metrics
        if let Some(obj) = result.metrics.as_object() {
            let skip_keys = ["success", "duration_ms", "error"];
            let additional: Vec<_> = obj
                .iter()
                .filter(|(k, _)| !skip_keys.contains(&k.as_str()))
                .collect();

            if !additional.is_empty() {
                detail.push_str("\n**Metrics:**\n");
                detail.push_str("```json\n");
                for (key, value) in additional {
                    detail.push_str(&format!("  \"{}\": {}\n", key, value));
                }
                detail.push_str("```\n");
            }
        }

        detail
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_generate_empty_report() {
        let gen = MarkdownGenerator::new();
        let md = gen.generate(&[]);

        assert!(md.contains("# Benchmark Results Summary"));
        assert!(md.contains("**Total Benchmarks:** 0"));
    }

    #[test]
    fn test_generate_with_results() {
        let gen = MarkdownGenerator::new();
        let results = vec![
            BenchmarkResult::success("test::a", 100),
            BenchmarkResult::success("test::b", 200),
            BenchmarkResult::failure("test::c", "Something went wrong"),
        ];

        let md = gen.generate(&results);

        assert!(md.contains("**Total Benchmarks:** 3"));
        assert!(md.contains("**Successful:** 2"));
        assert!(md.contains("**Failed:** 1"));
        assert!(md.contains("test::a"));
        assert!(md.contains("test::b"));
        assert!(md.contains("test::c"));
    }

    #[test]
    fn test_custom_config() {
        let config = MarkdownConfig {
            title: "Custom Report".to_string(),
            include_details: false,
            include_timestamp: false,
            use_tables: true,
        };

        let gen = MarkdownGenerator::with_config(config);
        let md = gen.generate(&[BenchmarkResult::success("test", 50)]);

        assert!(md.contains("# Custom Report"));
        assert!(!md.contains("## Details"));
    }
}
