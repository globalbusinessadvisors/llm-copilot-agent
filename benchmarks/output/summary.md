# Benchmark Results Summary

**Generated:** Initial template - Run benchmarks to populate with actual results

## Summary

- **Total Benchmarks:** 0 (pending execution)
- **Successful:** -
- **Failed:** -

## Available Benchmark Targets

Run `copilot benchmark list` to see all available benchmarks, or `copilot run` to execute all benchmarks.

### NLP Benchmarks
- `nlp::intent::simple` - Simple intent classification
- `nlp::intent::complex` - Complex multi-part query classification
- `nlp::intent::batch` - Batch intent classification

### Context Retrieval Benchmarks
- `context::retrieval::simple` - Simple context retrieval with varying k values
- `context::retrieval::large_corpus` - Large corpus retrieval performance

### Conversation Benchmarks
- `conversation::response::simple` - Simple response generation
- `conversation::multi_turn` - Multi-turn conversation handling

### Workflow Benchmarks
- `workflow::execution` - DAG-based workflow execution
- `workflow::validation` - Workflow definition validation

### Sandbox Execution Benchmarks
- `sandbox::python::execution` - Python code execution in E2B sandbox
- `sandbox::nodejs::execution` - Node.js code execution in E2B sandbox

### Ingestion Benchmarks
- `ingestion::document` - Document ingestion pipeline
- `ingestion::chunking` - Text chunking with varying sizes

### Observability Benchmarks
- `observability::metrics::collection` - Metrics collection and aggregation
- `observability::tracing` - Distributed tracing span creation

## Usage

```bash
# Run all benchmarks
copilot run

# Run benchmarks with filter
copilot run --filter nlp

# Run benchmarks in parallel
copilot run --parallel

# List available benchmarks
copilot benchmark list

# Show specific benchmark result
copilot benchmark show nlp::intent::simple
```

---

*Generated by copilot-benchmarks - Canonical Benchmark Interface*
